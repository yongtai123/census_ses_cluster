#!/usr/bin/env python
# coding: utf-8

# In[88]:


import copy
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import scale
from sklearn.preprocessing import minmax_scale
from scipy.special import rel_entr



def privacy_refinement(df, X, labels, pop_limit, min_tract, start_K, _pop_col, _tract_col,
                       show_progress=False):
    """
    Refine the clusters generated by clustering algo to make them satisfy
    privacy requirements.
    
    Parameters
    ----------
    df : pandas DataFrame
        orignal unclustered data
    X : numpy ndarray
        Data used for clustering
    labels : list
        cluster id of each point
    pop_limit : int
        the minimum number of populations in one cluster
    min_tract : int
        the minimum number of census tacts in one cluster  
    start_K : int
        Check for empty clusters
    show_progress : bool
        Print current processed nodes, default False.
    """
    
    df['cluster'] = labels
    # Some clusters has no tarcts (algo didn't converge?)
    # Cluster ids are not continuous, cannot be used as index to access clstr_pop. 
    # To fix it, map labels to continuous new labels 
    if len(np.unique(labels)) < start_K:
        lst = np.unique(labels)
        dct =dict(map(reversed, enumerate(lst))) # dict{label: index}
        df['cluster'] = df['cluster'].map(dct)
        labels = df['cluster']
        
    labels = copy.deepcopy(df['cluster'])
        
    cgb = df.groupby('cluster').agg({ _pop_col:'sum',
                                     _tract_col:'count'})
    cmask = (cgb[_pop_col] < pop_limit) | (cgb[_tract_col] < min_tract)
    print("Total number of Clusters : {}".format(len(cgb.index)))
    print("Clusters don't satisfy requriements: {}".format(len(cgb[cmask].index)))
    print("Merging clusters...")
    if show_progress:
        print(cgb.sort_values(by=[_pop_col,_tract_col],axis=0).head(5))
          
    # For old merge_cluster func
    # rank all the clusters based on population
    tmp = cgb.sort_values(by=[_pop_col,_tract_col],axis=0)
    tmask = (tmp[_pop_col] < pop_limit) | (tmp[_tract_col] < min_tract)
    splt_clstr =  tmp[tmask].index.to_numpy()
    clstr_pop = cgb[_pop_col].to_numpy()
    num_nodes = cgb[_tract_col].to_numpy()
    
    # For New merge_cluster func
    cgb.columns = ['pop','nodes']
    cgb['bad'] = False
    cgb.loc[cmask, 'bad'] = True
          
    centers = np.empty((len(np.unique(labels)),len(X[0])))
    for i in range(len(np.unique(labels))):
        centers[i] = X[df['cluster']==i].mean(axis=0)

    pops = df[_pop_col].to_numpy()
    
   
    labels = _merge_cluster(X, centers, pops, splt_clstr, clstr_pop, pop_limit,
                             num_nodes, min_tract, labels, show_progress)
    
    df.drop(columns='cluster',inplace=True)
    # make labels continuous
    lst = np.unique(labels)
    dct =dict(map(reversed, enumerate(lst)))
    labels = [dct[i] for i in labels]
    print("Final number of Clusters : {}".format(len(np.unique(labels))))
    return labels


def _merge_cluster(X, centers, pops, splt_clstr, clstr_pop, pop_limit, 
                  num_nodes, min_tract, org_labels, SHOW):
    """
    Merge clusters that do not satisfy privacy requirement.

    """

    for i in splt_clstr:
        if SHOW:
            print("current split cluster: {}".format(i))

        #if current clstr has a population greater than pop_limit, skip this one
        if (clstr_pop[i] >= pop_limit) and (num_nodes[i] >= min_tract):
            if SHOW:
                print('good one!')
            continue

        # find all nodes in current cluster
        Nodes = np.where(org_labels == i)[0]
        if SHOW:
            print("NODES \n ", Nodes)

        # since we decide to split this cluster,
        # make this cluster's center inifinity large, 
        # so it won't affect the distance calculation in next steps
        centers[i] = np.full(len(centers[i]), np.inf)
    
        for node in Nodes:

            #print('node \n',node)
            node = int(node)
            attr = X[node] # get attributes for cur node
            p = pops[node] # get population of cur node
            #print(attr, p)

            dist_2 = np.sum((centers - attr)**2, axis=1) # calculate the distance between current node and all the centers
            idd = np.argmin(dist_2) #get the index of the current closest cluster
            #print("merged center {}".format(idd))
            #print("old cetroid: {}".format(centers[idd]))

            # add this node to closest cluster
            org_labels[node] = idd

            # update cluster's centroid
            centers[idd] = (num_nodes[idd]*centers[idd] + attr) /(num_nodes[idd]+1)

            # update num of nodes in cluster
            num_nodes[idd] += 1
            num_nodes[i] -= 1
            #print("new cetroid: {}".format(centers[idd]))

            # update cluster's population
            clstr_pop[idd] += p
            clstr_pop[i] -= p
            #print()
    return org_labels


def save_result(df, labels, features, _pop_col, _tract_col, outputcsv, prefix='cluster_'):
    """
    Calculate deprivation index of clustered dataset, save clustered data to outputcsv. 
    
    Parameters
    ----------
    df : pandas.DataFrame
    labels : string
    features : list
        features to copmare
    _pop_col : string
    _tract_col : string
        column name of census tract id column
    outputcsv : string
        output file destination
    prefix : string
        Prefix for features in the clustered file, default is 'cluster_'.

    Return
    ------
    """
    
    pca=PCA(n_components=1)
    X=pca.fit_transform(scale(df[features].to_numpy()))
    old_dep = X[:,0]
    
    df['cluster'] = labels
    # Get statisitcs
    aggdct = {_pop_col:'sum', _tract_col:'count'}
    for i in features: 
        aggdct[i] = 'mean'
    cgb = df.groupby('cluster').agg(aggdct)    
    
    mean_feature = [ x +'_mean' for x in features]
    cgb.rename(columns={i:i+'_mean' for i in features},inplace=True)
    df = df.join(cgb[mean_feature], on='cluster')

    pca=PCA(n_components=3)
    X=pca.fit_transform(scale(df[mean_feature].to_numpy()))
    pc1 = X[:,0]
    
    mindep = min(old_dep)
    maxdep = max(old_dep)
    pc1 = (pc1 - mindep) / (maxdep - mindep)
    pc1[pc1>1] = 1
    pc1[pc1<0] = 0
    
    newdep = prefix+'dep_index'
    df[newdep] = pc1
    
    c_features = [prefix+ x for x in features]
    dict_col_name = dict(zip(mean_feature, c_features))
    df.rename(columns=dict_col_name,inplace=True)
    df.drop(columns=features,inplace=True)
    df.drop(columns=['dep_index'],inplace=True)
    
    # reorder the column list
    ordered_col = [_tract_col, _pop_col, 'cluster']
    for i in c_features:
        ordered_col.append(i)
    ordered_col.append(newdep)
    df = df[ordered_col]
    df.to_csv(outputcsv, index=False)


# In[86]:


def kl_divergence(orignalcsv, clusteredcsv, features, _tract_col, prefix='cluster_'):
    """
    Calculate the KL divergence between orignal and clustered data
    
    Parameters
    ----------
    orignalcsv : string
    clusteredcsv : string
    features : list
        features to copmare
    _tract_col : string
        column name of census tract id column
    prefix : string
        Prefix for features in the clustered file, default is 'cluster_'.

    Return
    ------
    kldivs : list
        The KL-div for each feature, with the sum of all features appended at the end.
    """

    odf = pd.read_csv(orignalcsv)
    cdf = pd.read_csv(clusteredcsv)
    if len(odf) != len(cdf):
        print("WARNING! orignal file and clustered one has different number of tracts,")
        print("run the comparison over the common tracts of two")
        cmn_tracts = list(set(odf.census_tract_fips).intersection(set(cdf.census_tract_fips)))
        odf = copy.deepcopy(odf.set_index(_tract_col).loc[cmn_tracts])
        cdf = copy.deepcopy(cdf.set_index(_tract_col).loc[cmn_tracts])
        
        
        
    kldivs = []
    for f in features:
        minf = min(odf[f])
        maxf = max(odf[f])
        of = minmax_scale(odf[f])
        cf = (cdf[prefix+f] - minf) / (maxf - minf)
        cf[cf==0] = 0.00001 # KL div doesnt' allow y to be zero
        div = sum(rel_entr(of,cf))
        kldivs.append(div)
        print("KL divergenc of feature {} is {}".format(f,div))
        
    of = copy.deepcopy(odf['dep_index'])
    cf = copy.deepcopy(cdf[prefix+'dep_index'])
    of[of==0] = 0.00001
    cf[cf<=0] = 0.00001
    div = sum(rel_entr(of,cf))
    kldivs.append(div)
    print("KL divergenc of feature dep_index is {}".format(div))
    
    kldivs.append(sum(kldivs))
    print("KL divergenc of all feature is {}".format(kldivs[-1]))
    return kldivs

